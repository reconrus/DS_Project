{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UNMT.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "3hwcoWrtk_PC"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMXQE3K4E6GdtstmkjxGZh5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reconrus/DS_Project/blob/dev/UNMT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srF9AmoNjloE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['PROJECT_PATH'] = os.path.abspath(os.curdir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzEsS8lkjnSW",
        "colab_type": "text"
      },
      "source": [
        "**Mount Google Drive**\n",
        "\n",
        "It looks like it is impossible to use Google Colab, since I am using torchtext package, where field.build_vocab method is broken for the latest version supported by python3.6, while in python3.7 everything is OK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iQRfEmrjmR_",
        "colab_type": "code",
        "outputId": "100a81eb-0e03-4943-ac20-9d696df4d484",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "from google.colab import drive\n",
        "os.environ['PROJECT_PATH']='/content/ydrive/My Drive/Study/UNMT'\n",
        "drive.mount('/content/ydrive/')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/ydrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g02rnRLKjmP5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.environ['TOOLS']= os.path.join(os.environ['PROJECT_PATH'], 'tools')\n",
        "os.environ['RESOURCES']= os.path.join(os.environ['PROJECT_PATH'], 'resources')\n",
        "os.environ['DATA']= os.path.join(os.environ['RESOURCES'], 'data')\n",
        "os.environ['MODELS']= os.path.join(os.environ['PROJECT_PATH'], 'models')\n",
        "\n",
        "# DATA VARIABLES\n",
        "os.environ['VOCAB_SIZE']=\"60000\"\n",
        "os.environ['L1']='ba'\n",
        "os.environ['L2']='ru'\n",
        "os.environ['L1_DATA']=\"ba.sentesized\"  \n",
        "os.environ['L2_DATA']=\"news.2016.ru.shuffled\"\n",
        "os.environ['L1_DATA_PREPARED']=os.path.join(os.environ['DATA'], \"{}.{}\".format(os.environ['L1_DATA'], os.environ['VOCAB_SIZE']))\n",
        "os.environ['L2_DATA_PREPARED']=os.path.join(os.environ['DATA'], \"{}.{}\".format(os.environ['L2_DATA'], os.environ['VOCAB_SIZE']))\n",
        "\n",
        "os.environ[\"EMBEDDINGS_DIR\"]=os.path.join(os.environ[\"RESOURCES\"], \"embeddings\")\n",
        "os.environ[\"BPE_EMBEDDINGS\"]=\"crosslingualbpe\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Th28wv8si8DZ",
        "colab_type": "text"
      },
      "source": [
        "# Download data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fv568vkYjBfh",
        "colab_type": "text"
      },
      "source": [
        "## Bashkir Language (source)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aMb_SkZi5Qx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "BASHKIR=\"$DATA/bashkir\"\n",
        "git clone https://github.com/nevmenandr/bashkir-corpus \"$BASHKIR-corpus\"\n",
        "mkdir \"$BASHKIR\" & mkdir \"$BASHKIR/raw\"\n",
        "find \"$BASHKIR-corpus\" -name \"*.txt\" -print0 | xargs -0 -I file cat file > \"$BASHKIR/ba\"\n",
        "# rm -rf -d  \"$BASHKIR-corpus\"\n",
        "\n",
        "WIKIEXTRACTOR=\"$TOOLS/wikiextractor\"\n",
        "git clone https://github.com/ptakopysk/wikiextractor \"$WIKIEXTRACTOR\"\n",
        "[ -f $BASHKIR/bawiki-latest-pages-articles.xml.bz2 ] || wget http://download.wikimedia.org/bawiki/latest/bawiki-latest-pages-articles.xml.bz2 -P \"$BASHKIR\"\n",
        "python3 \"$WIKIEXTRACTOR/WikiExtractor.py\"  --json -o \"$BASHKIR/ba_wiki\" \"$BASHKIR/bawiki-latest-pages-articles.xml.bz2\"\n",
        "# rm \"$BASHKIR/bawiki-latest-pages-articles.xml.bz2\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsoRcC-kjRWJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "\n",
        "input_folder = os.path.join(os.environ['DATA'], 'bashkir', 'ba_wiki')\n",
        "output_path = os.path.join(os.environ['DATA'], 'bashkir', 'ba')\n",
        "\n",
        "output_file = open(output_path, \"a+\", encoding='utf-8')\n",
        "\n",
        "for path, subdirs, files in os.walk(input_folder):\n",
        "    for name in files:\n",
        "        file = open(os.path.join(path, name), 'r', encoding='utf-8')\n",
        "        for line in file.readlines():\n",
        "            dump = json.loads(line)\n",
        "            if dump[\"text\"].strip('\\n'):\n",
        "              output_file.write(\"%s\\n\" % dump[\"text\"])\n",
        "        file.close()\n",
        "\n",
        "output_file.close()\n",
        "\n",
        "# !rm -rf -d \"$DATA/bashkir/ba_wiki\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrekFRqFjTkt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install razdel\n",
        "from razdel import sentenize\n",
        "\n",
        "raw_data_path = os.path.join(os.environ['DATA'], 'bashkir', 'ba')\n",
        "sentenized_data_path = os.path.join(os.environ['DATA'], 'ba.sentesized')\n",
        "\n",
        "raw_data = open(raw_data_path, 'r', encoding='utf-8')\n",
        "sentenized_data = open(sentenized_data_path, 'w+', encoding='utf-8')\n",
        "\n",
        "for line in raw_data:\n",
        "    sentences = sentenize(line)\n",
        "    sentenized_data.writelines([\"%s\\n\" % sentence.text for sentence in sentences])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6anyB6mMjdVc",
        "colab_type": "text"
      },
      "source": [
        "## Russian Language"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ldp5znAljcrw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "wget http://data.statmt.org/wmt17/translation-task/news.2016.ru.shuffled.gz -P \"$DATA\"\n",
        "gzip -d \"$DATA/news.2016.ru.shuffled.gz\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHrDj0mjk7I1",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hwcoWrtk_PC",
        "colab_type": "text"
      },
      "source": [
        "## Text cleaning and tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1k8C04l6k6eR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U sacremoses\n",
        "from sacremoses import MosesPunctNormalizer, MosesTokenizer\n",
        "\n",
        "def preprocess_file(filepath, language):\n",
        "    normalizer = MosesPunctNormalizer(language, pre_replace_unicode_punct=True, post_remove_control_chars=True)\n",
        "    tokenizer = MosesTokenizer(language)\n",
        "    output_file = open('%s.cleaned' % filepath, 'w+', encoding='utf-8')\n",
        "\n",
        "    with open(filepath, 'r', encoding='utf-8') as input_file:\n",
        "        for line in input_file:\n",
        "            line = normalizer.normalize(line)\n",
        "            tokens = tokenizer.tokenize(line)\n",
        "            output_file.write(\"{}\\n\".format(' '.join(tokens)))\n",
        "\n",
        "\n",
        "preprocess_file(os.path.join(os.environ['DATA'], os.environ['L1_DATA']), os.environ['L1']) \n",
        "preprocess_file(os.path.join(os.environ['DATA'], os.environ['L2_DATA']), os.environ['L2'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCz9QlkQlqGx",
        "colab_type": "text"
      },
      "source": [
        "## BPE codes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmJe-ViKloM-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "FASTBPE=\"$TOOLS/fastBPE\"\n",
        "FAST=\"$FASTBPE/fast\"\n",
        "git clone https://github.com/glample/fastBPE \"$FASTBPE\"\n",
        "g++ -std=c++11 -pthread -O3 \"$FASTBPE/fastBPE/main.cc\" -IfastBPE -o \"$FAST\"\n",
        "\"$FAST\" learnbpe $VOCAB_SIZE \"$DATA/${L1_DATA}.cleaned\" \"$DATA/${L2_DATA}.cleaned\" > \"$DATA/BPE_codes\"\n",
        "\"$FAST\" applybpe \"$DATA/${L1_DATA_PREPARED}\" \"$DATA/${L1_DATA}.cleaned\" \"$DATA/BPE_codes\"\n",
        "\"$FAST\" applybpe \"$DATA/${L2_DATA_PREPARED}\" \"$DATA/${L2_DATA}.cleaned\" \"$DATA/BPE_codes\"\n",
        "\n",
        "\"$FAST\" getvocab \"$DATA/${L1_DATA_PREPARED}\" > \"$DATA/vocab.${L1_DATA}.60000\" \n",
        "\"$FAST\" getvocab \"$DATA/${L2_DATA_PREPARED}\" > \"$DATA/vocab.${L2_DATA}.60000\" \n",
        "\"$FAST\" getvocab \"$DATA/${L1_DATA_PREPARED}\" \"$DATA/${L2_DATA_PREPARED}\" > \"$DATA/vocab.full.60000\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDvvI34mzPnq",
        "colab_type": "text"
      },
      "source": [
        "## Cross-lingual Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rh2I-y4GzPK4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "FASTTEXT_DIR=\"$TOOLS/fastText\"\n",
        "FASTTEXT=\"$FASTTEXT_DIR/fasttext\"\n",
        "git clone https://github.com/facebookresearch/fastText.git \"$FASTTEXT_DIR\"\n",
        "cd \"$FASTTEXT_DIR\" \n",
        "[ -f \"$FASTTEXT\" ] || make\n",
        "\n",
        "CONCAT_BPE=\"$DATA/concatenated.6000\"\n",
        "N_THREADS=$(grep -c ^processor /proc/cpuinfo)\n",
        "echo $N_THREADS\n",
        "cat \"$DATA/${L1_DATA_PREPARED}\" \"$DATA/${L2_DATA_PREPARED}\" | shuf > \"$CONCAT_BPE\"\n",
        "chmod +x \"$FASTTEXT\"\n",
        "\"$FASTTEXT\" skipgram -dim 256 -thread $N_THREADS -input \"$CONCAT_BPE\" -output \"$EMBEDDINGS_DIR/$BPE_EMBEDDINGS\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94T9Avacpk8A",
        "colab_type": "text"
      },
      "source": [
        "# Model implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHzKdvLgplk_",
        "colab_type": "text"
      },
      "source": [
        "## Tools"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdN55Q2aEkyi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy\n",
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "#src https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "def get_module_clones(module, N):\n",
        "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
        "\n",
        "def get_mask(inputs, pad_mask): #[2]\n",
        "    slen, bs = inputs.size()\n",
        "    lengths = slen-torch.sum(pad_mask, 0)\n",
        "    alen = torch.arange(slen, dtype=torch.long, device=lengths.device)\n",
        "    return alen < lengths[:, None]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EB7Wv-lppYL",
        "colab_type": "text"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wWOxh8XtXCF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn.functional as F\n",
        "from torch.nn import TransformerDecoder, TransformerDecoderLayer, \\\n",
        "                     TransformerEncoder, TransformerEncoderLayer\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  \n",
        "    def __init__(self, field, d_model=256, nlayers=4, nheads=8, dropout=0.1, shared_nlayers=2, freeze_embs=True):\n",
        "        \"\"\"\n",
        "        :param share_n_layers: number of layers that are shared for both languages\n",
        "        \"\"\"\n",
        "        super(Encoder, self).__init__()\n",
        "        \n",
        "        self.voc_size = len(field.vocab) \n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.dropout = dropout\n",
        "        self.embeddings = nn.Embedding(self.voc_size, d_model).from_pretrained(field.vocab.vectors, freeze=freeze_embs)\n",
        "        self.pos_encoder = PositionalEncoding(d_model)\n",
        "        encoder_layer = TransformerEncoderLayer(d_model, nheads, dim_feedforward=4*d_model, dropout=dropout, activation='gelu')\n",
        "        self.layers = nn.ModuleList()\n",
        "        # Layers for the source language\n",
        "        self.layers.append(get_module_clones(encoder_layer, nlayers-shared_nlayers))\n",
        "        # Layers for the target language\n",
        "        self.layers.append(get_module_clones(encoder_layer, nlayers-shared_nlayers))\n",
        "        \n",
        "        shared_layers = get_module_clones(encoder_layer, shared_nlayers)\n",
        "        self.layers[0].extend(shared_layers)\n",
        "        self.layers[1].extend(shared_layers)\n",
        "\n",
        "\n",
        "    def forward(self, src, pad_mask, lang_id):\n",
        "        src_mask = get_mask(src, pad_mask)\n",
        "        x = self.embeddings(src)\n",
        "        x = self.pos_encoder(x)\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "      \n",
        "        for layer in self.layers[lang_id]:\n",
        "            x = layer(x, src_mask, pad_mask)\n",
        "\n",
        "        return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIPdFq6Yw-E3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, field, encoder, d_model=256, nlayers=4, nheads=8, dropout=0.1, shared_nlayers=2):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.sos_idx = field.vocab.stoi['<sos>']\n",
        "        self.eos_idx = field.vocab.stoi['<eos>']\n",
        "        self.pad_idx = field.vocab.stoi['<pad>']\n",
        "        self.d_model = d_model\n",
        "        self.dropout = dropout\n",
        "        self.embeddings = encoder.embeddings\n",
        "        self.pos_encoder = PositionalEncoding(d_model)\n",
        "\n",
        "        decoder_layer = TransformerDecoderLayer(d_model, nheads, dim_feedforward=4*d_model, dropout=dropout, activation='gelu')\n",
        "        self.layers = nn.ModuleList()\n",
        "        shared_layers = get_module_clones(decoder_layer, shared_nlayers)\n",
        "        # Layers for the source language with shared bottom layers\n",
        "        self.layers.append(shared_layers) \n",
        "        self.layers[0].extend(get_module_clones(decoder_layer, nlayers-shared_nlayers))\n",
        "        # Layers for the target language with shared bottom layers\n",
        "        self.layers.append(shared_layers) \n",
        "        self.layers[1].extend(get_module_clones(decoder_layer, nlayers-shared_nlayers))\n",
        "\n",
        "\n",
        "        # share proj_layer \n",
        "        proj_layer = nn.Linear(self.embeddings.embedding_dim, len(field.vocab))\n",
        "        proj_layers = [proj_layer]*2\n",
        "        # proj_layers = [nn.Linear(self.emb_dim, len(vocab)) for vocab in vocabs]\n",
        "        self.proj_layers = nn.ModuleList(proj_layers)\n",
        "\n",
        "    def forward(self, previous_tokens, encoded, enc_pad_mask, lang_id):\n",
        "        x = self.embeddings(previous_tokens)\n",
        "        x = self.pos_encoder(x)\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "      \n",
        "        for layer in self.layers[lang_id]:\n",
        "            x = layer(x, encoded, memory_key_padding_mask=enc_pad_mask)\n",
        "\n",
        "        x = self.proj_layers[lang_id](x)\n",
        "        return x\n",
        "\n",
        "    def generate_sequence(self, encoded, enc_pad_mask, lang_id, sequence_len=256):\n",
        "        cur_len = 1\n",
        "        bs = encoded.size(1)\n",
        "        decoded = torch.LongTensor(sequence_len, bs).fill_(self.pad_idx)\n",
        "        decoded = decoded.to(encoded.device)\n",
        "        decoded[0] = self.sos_idx\n",
        "\n",
        "        unfinished_sents = torch.LongTensor(bs).fill_(1)\n",
        "        \n",
        "        while cur_len < sequence_len:\n",
        "            scores = self.forward(decoded[:cur_len], encoded, enc_pad_mask, lang_id)\n",
        "            scores = scores.detach()[-1, :, :]\n",
        "            next_words = torch.topk(scores, 1)[1].squeeze(1)\n",
        "            assert next_words.size() == (bs,)\n",
        "\n",
        "            decoded[cur_len] = next_words*unfinished_sents + self.pad_idx*(1-unfinished_sents)\n",
        "            unfinished_sents.mul_(next_words.ne(self.eos_idx).long())\n",
        "            cur_len += 1\n",
        "\n",
        "            if unfinished_sents.max() == 0:\n",
        "                break\n",
        "\n",
        "        \n",
        "        if cur_len == sequence_len:\n",
        "            decoded[sequence_len - 1].masked_fill_(unfinished_sents.byte(), self.eos_idx)\n",
        "\n",
        "        return decoded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbszYcXVGz8k",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxJWr0cQVRSB",
        "colab_type": "text"
      },
      "source": [
        "1) https://github.com/pytorch/fairseq/blob/7b3df95f287bc0d844f64fe45717123d06dacb97/fairseq/data/noising.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fAdRiV3bFw4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "# [1]\n",
        "class Noising:\n",
        "    def __init__(self, vocab):\n",
        "        self.vocav = vocab\n",
        "        self.bpe_ends_mask = np.array([not vocab.itos[i].endswith('@@') for i in range(len(vocab))])\n",
        "        \n",
        "        self.pad_idx = vocab.stoi['<pad>']\n",
        "        self.sos_idx = vocab.stoi['<sos>']\n",
        "        self.eos_idx = vocab.stoi['<eos>']\n",
        "        self.sep_idx = vocab.stoi['<sep>']\n",
        "        self.mask_idx = vocab.stoi['<mask>']\n",
        "\n",
        "    def noise(self, inp):\n",
        "        x = inp.cpu()\n",
        "        pad_mask = x.eq(self.pad_idx)\n",
        "        lengths = x.size(0) - pad_mask.sum(0)\n",
        "\n",
        "        x = self.shuffle(x, lengths)\n",
        "        x, lengths = self.dropout(x, lengths)\n",
        "        x, lengths = self.dropout(x, lengths, blank_idx=self.mask_idx)\n",
        "        return x\n",
        "\n",
        "    def get_word_idx(self, x):\n",
        "        bpe_end = self.bpe_ends_mask[x]\n",
        "        word_idx = bpe_end[::-1].cumsum(0)[::-1]\n",
        "        word_idx = word_idx.max(0)[None, :] - word_idx \n",
        "        return word_idx\n",
        "\n",
        "    def dropout(self, x, lengths, dropout_rate=0.1, blank_idx=None):\n",
        "        sentences = []\n",
        "        modified_lengths = []\n",
        "        word_idx = self.get_word_idx(x)\n",
        "        sos_mask = x.eq(self.sos_idx)\n",
        "        eos_mask = x.eq(self.eos_idx)\n",
        "        not_dropout_mask = sos_mask + eos_mask\n",
        "        not_dropout_mask = not_dropout_mask.numpy()\n",
        "        \n",
        "        for i in range(lengths.size(0)):\n",
        "            num_words = max(word_idx[:, i]) + 1\n",
        "            keep = np.random.rand(num_words) >= dropout_rate\n",
        "            do_not_dropout_words_idx = word_idx[:, i]*not_dropout_mask[:, i]\n",
        "            keep[do_not_dropout_words_idx] = 1 # do not dropout <sos> symbol\n",
        "            words = x[:lengths[i], i].tolist()\n",
        "            new_s = [\n",
        "                w if keep[word_idx[j, i]] else blank_idx\n",
        "                for j, w in enumerate(words)\n",
        "            ]\n",
        "            new_s = [w for w in new_s if w is not None]\n",
        "            sentences.append(new_s)\n",
        "            modified_lengths.append(len(new_s))\n",
        "        # re-construct input\n",
        "        modified_lengths = torch.LongTensor(modified_lengths)\n",
        "\n",
        "        modified_x = torch.LongTensor(\n",
        "            x.size(0),\n",
        "            x.size(1)\n",
        "        ).fill_(self.pad_idx)\n",
        "        for i in range(modified_lengths.size(0)):\n",
        "            modified_x[:modified_lengths[i], i].copy_(torch.LongTensor(sentences[i]))\n",
        "\n",
        "        return modified_x, modified_lengths\n",
        "\n",
        "    def shuffle(self, x, lengths, max_shuffle_distance=3):\n",
        "        if max_shuffle_distance == 0:\n",
        "            return x\n",
        "        eos_mask = x.eq(self.eos_idx)\n",
        "        lengths -= eos_mask.sum(0)\n",
        "\n",
        "        noise = np.random.uniform(\n",
        "            0,\n",
        "            max_shuffle_distance,\n",
        "            size=(x.size(0), x.size(1)),\n",
        "        )\n",
        "        \n",
        "        sos_mask = x.eq(self.sos_idx).numpy()\n",
        "        do_not_shuffle_indices = np.nonzero(sos_mask)\n",
        "        noise[do_not_shuffle_indices] = -1 # do not move <sos> symbols\n",
        "        word_idx = self.get_word_idx(x)\n",
        "\n",
        "        x2 = x.clone()\n",
        "        for i in range(lengths.size(0)):\n",
        "            scores = word_idx[:lengths[i], i] + noise[word_idx[:lengths[i], i], i]\n",
        "            scores += 1e-6 * np.arange(lengths[i])\n",
        "            permutation = scores.argsort()\n",
        "            x2[:lengths[i], i].copy_(\n",
        "                x2[:lengths[i], i][torch.from_numpy(permutation)]\n",
        "            )\n",
        "        return x2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkWfxbXOvUGB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import OrderedDict\n",
        "from torch.optim import Adam\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, encoder, decoder, field, params, logger, clip=1.0, lr=0.0001, n_iter=0, n_epoch=0):\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "        self.pad_idx = field.vocab.stoi['<pad>']\n",
        "        self.vocab_size = len(field.vocab)\n",
        "\n",
        "        self.criterion = nn.CrossEntropyLoss(ignore_index=self.pad_idx)\n",
        "        self.clip = clip\n",
        "        self.n_updates = n_iter\n",
        "\n",
        "        self.noising = Noising(field.vocab)\n",
        "\n",
        "        self.enc_optimizer = Adam(encoder.parameters(), lr=lr)\n",
        "        self.dec_optimizer = Adam(decoder.parameters(), lr=lr)\n",
        "\n",
        "        self.n_total_iter = n_iter\n",
        "        self.epoch = n_epoch \n",
        "\n",
        "        self.device = params.device\n",
        "        self.logger = logger\n",
        "\n",
        "    def get_denoising_loss_weight(self, init_weight=1, decrease_slower_iter=10**5, weight_slower=0.1, set_to_zero_iter=3*10**5):\n",
        "        if self.n_updates < decrease_slower_iter:\n",
        "            return init_weight - ((init_weight-weight_slower)/decrease_slower_iter)*self.n_updates\n",
        "\n",
        "        return weight_slower - (weight_slower/(set_to_zero_iter - decrease_slower_iter))*(self.n_updates - decrease_slower_iter)\n",
        "    \n",
        "    def backprop(self, loss):\n",
        "        self.enc_optimizer.zero_grad()\n",
        "        self.dec_optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(self.encoder.parameters(), self.clip)\n",
        "        nn.utils.clip_grad_norm_(self.decoder.parameters(), self.clip)\n",
        "        self.enc_optimizer.step()\n",
        "        self.dec_optimizer.step()\n",
        "\n",
        "    def denoising_step(self, inp, lang_id):\n",
        "        x = self.noising.noise(inp).to(self.device)\n",
        "        pad_mask = x.eq(self.pad_idx).transpose_(0, 1)\n",
        "        self.encoder.train()\n",
        "        self.decoder.train()\n",
        "        encoded = self.encoder(x, pad_mask, lang_id)\n",
        "        scores = self.decoder(x[:-1], encoded, pad_mask, lang_id)\n",
        "        loss = self.criterion(scores.view(-1, self.vocab_size), inp[1:].view(-1))\n",
        "\n",
        "        loss = self.get_denoising_loss_weight() * loss\n",
        "\n",
        "        # check NaN\n",
        "        if (loss != loss).data.any():\n",
        "            print(\"NaN detected\")\n",
        "            exit()\n",
        "\n",
        "        self.backprop(loss)\n",
        "\n",
        "        self.n_updates += 1\n",
        "        progress_state = OrderedDict(\n",
        "            step_type='denoising',\n",
        "            loss=loss.item(),\n",
        "            sentences=inp.size(1),\n",
        "            n_total_iter=self.n_total_iter,\n",
        "            epoch=self.epoch,\n",
        "            lang_id=lang_id\n",
        "            )\n",
        "\n",
        "        return progress_state\n",
        "\n",
        "    def backtranslation_step(self, src, trg, src_lang_id, trg_lang_id):\n",
        "        # src -> trg -> src\n",
        "        self.encoder.train()\n",
        "        self.decoder.train()\n",
        "\n",
        "        trg_pad_mask = trg.eq(self.pad_idx).transpose_(0, 1)\n",
        "        encoded = self.encoder(trg, trg_pad_mask, trg_lang_id)\n",
        "\n",
        "        src_pad_mask = src.eq(self.pad_idx).transpose_(0, 1)\n",
        "        scores = self.decoder(src[:-1], encoded, src_pad_mask, src_lang_id)\n",
        "        loss = self.criterion(scores.view(-1, self.vocab_size), src[1:].view(-1))\n",
        "\n",
        "        # check NaN\n",
        "        if (loss != loss).data.any():\n",
        "            print(\"NaN detected\")\n",
        "            exit()\n",
        "\n",
        "        self.backprop(loss)\n",
        "\n",
        "        progress_state = OrderedDict(\n",
        "            step_type='backtranslation',\n",
        "            loss=loss.item(),\n",
        "            sentences=src.size(1),\n",
        "            n_total_iter=self.n_total_iter,\n",
        "            epoch=self.epoch,\n",
        "            backtranslation_direction='{}->{}->{}'.format(src_lang_id, trg_lang_id, src_lang_id)\n",
        "            )\n",
        "        \n",
        "        return progress_state\n",
        "\n",
        "    def generate_translation(self, src, lang1_id, lang2_id):\n",
        "        self.encoder.eval()\n",
        "        self.decoder.eval()\n",
        "\n",
        "        pad_mask = src.eq(self.pad_idx).transpose_(0, 1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            encoded = self.encoder(src, pad_mask, lang1_id)          \n",
        "            trg = self.decoder.generate_sequence(encoded, pad_mask, lang2_id)\n",
        "\n",
        "        return trg\n",
        "\n",
        "    def save_model(self, dump_dir, name):\n",
        "        path = os.path.join(dump_dir, '%s.pth' % name)\n",
        "        self.logger.log('Saving model to %s ...' % path)\n",
        "        torch.save({\n",
        "            'encoder': self.encoder,\n",
        "            'decoder': self.decoder,\n",
        "            'enc_optimizer': self.enc_optimizer,\n",
        "            'dec_optimizer': self.dec_optimizer,\n",
        "            'epoch': self.epoch,\n",
        "            'n_total_iter': self.n_total_iter, \n",
        "            'criterion': self.criterion\n",
        "        }, path)\n",
        "\n",
        "    def reload_checkpoint(self, dump_dir, name):\n",
        "        checkpoint_path = os.path.join(dump_dir, name)\n",
        "        if not os.path.isfile(checkpoint_path):\n",
        "            return\n",
        "\n",
        "        print('Reloading checkpoint from %s ...' % checkpoint_path)\n",
        "        checkpoint_data = torch.load(checkpoint_path)\n",
        "        self.encoder = checkpoint_data['encoder']\n",
        "        self.decoder = checkpoint_data['decoder']\n",
        "        self.enc_optimizer = checkpoint_data['enc_optimizer']\n",
        "        self.dec_optimizer = checkpoint_data['dec_optimizer']\n",
        "        self.epoch = checkpoint_data['epoch']\n",
        "        self.n_total_iter = checkpoint_data['n_total_iter']\n",
        "        # self.best_metrics = checkpoint_data['best_metrics']\n",
        "        # self.best_stopping_criterion = checkpoint_data['best_stopping_criterion']\n",
        "        self.criterion = checkpoint_data['criterion']\n",
        "\n",
        "        self.logger.log('Checkpoint reloaded. Resuming at epoch %i ...' % self.epoch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Heohu9eLfie1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext import data\n",
        "\n",
        "class CustomDataset(data.Dataset):\n",
        "    def __init__(self, path, text_field, newline_eos=True,\n",
        "                 encoding='utf-8', **kwargs):\n",
        "        fields = [('text', text_field)]\n",
        "        with open(path, encoding=encoding) as f:\n",
        "            sentences = [text_field.preprocess(line) for line in f if line.strip('\\n')]        \n",
        "        examples = [data.Example.fromlist([sentence], fields) for sentence in sentences]\n",
        "        super(CustomDataset, self).__init__(\n",
        "            examples, fields, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGK7SE2MGyzn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from itertools import zip_longest\n",
        "from torchtext.datasets import LanguageModelingDataset\n",
        "from torchtext.vocab import Vectors\n",
        "\n",
        "class Logger:\n",
        "    def __init__(self, path=None):\n",
        "          self.log_file = open(path, 'a+') if path else None\n",
        "\n",
        "    def log(self, info):\n",
        "        if type(info) is str:\n",
        "            print(\"%s\\n\" % info, file=self.log_file)\n",
        "        elif type(info) is OrderedDict:\n",
        "            for k, v in info.items()\n",
        "                print(\"%s: \" % str(k), file=self.log_file)\n",
        "                print(\"%s\\n\" % str(v), file=self.log_file)\n",
        "\n",
        "            print('\\n\\n\\n', file=self.log_file)\n",
        "\n",
        "    def close(self):\n",
        "        if self.log_file:\n",
        "            self.log_file.close()\n",
        "\n",
        "\n",
        "def main(params):\n",
        "    logger = Logger()\n",
        "\n",
        "    TEXT = data.Field(\n",
        "        init_token='<sos>',\n",
        "        eos_token='<eos>',\n",
        "        fix_length=params.sequence_length\n",
        "    )\n",
        "    logger.log(\"Loaded Field\")\n",
        "    # train_l1_dataset = LanguageModelingDataset(params.l1_data_path, TEXT)\n",
        "    train_l1_dataset = CustomDataset(params.l1_data_path, TEXT)\n",
        "    logger.log(\"Loaded L1 Dataset\")\n",
        "    # train_l1_dataset = LanguageModelingDataset(params.l1_data_path, TEXT)\n",
        "    train_l2_dataset = CustomDataset(params.l2_data_path, TEXT)\n",
        "    logger.log(\"Loaded L2 Dataset\")\n",
        "    \n",
        "    vectors = Vectors(name=params.embs_file, cache=params.embs_dir)\n",
        "    logger.log(\"Loaded Vectors\")\n",
        "\n",
        "    TEXT.build_vocab(\n",
        "        train_l1_dataset,\n",
        "        train_l2_dataset,\n",
        "        specials=['<sep>', '<mask>'],\n",
        "        vectors=vectors\n",
        "    )\n",
        "    logger.log(\"Loaded Vocab\")\n",
        "\n",
        "    # DEBUG\n",
        "    print(TEXT.vocab['<pad>'])\n",
        "    print(TEXT.vocab['<sep>'])\n",
        "    print(TEXT.vocab['<mask>'])\n",
        "\n",
        "    l1_iter = data.BucketIterator(\n",
        "          dataset = train_l1_dataset,\n",
        "          batch_size = params.batch_size,\n",
        "          repeat=True,\n",
        "          shuffle=True,\n",
        "          device=params.device\n",
        "    )\n",
        "    logger.log(\"Created L1 Iterator\")\n",
        "\n",
        "    l2_iter = data.BucketIterator(\n",
        "          dataset = train_l2_dataset,\n",
        "          batch_size = params.batch_size,\n",
        "          repeat=True,\n",
        "          shuffle=True,\n",
        "          device=params.device\n",
        "    )\n",
        "    logger.log(\"Created L2 Iterator\")\n",
        "\n",
        "    encoder = Encoder(TEXT)\n",
        "    logger.log(\"Created Encoder\")\n",
        "    decoder = Decoder(TEXT, encoder)\n",
        "    logger.log(\"Created Decoder\")\n",
        "\n",
        "    trainer = Trainer(encoder, decoder, TEXT, params, logger)\n",
        "    logger.log(\"Created Trainer\")\n",
        "\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "\n",
        "    logger.log(\"===================TRAINING STARTED===================\")\n",
        "    while trainer.epoch <= params.n_epoch:\n",
        "        logger.log(\"===================EPOCH%d===================\" % trainer.epoch)\n",
        "        for batches in zip_longest(l1_iter, l2_iter, fillvalue=None):\n",
        "            for src_id in [0, 1]:\n",
        "                if not batches[src_id]: # if there is no batches for this language\n",
        "                    continue\n",
        "                trg_id = 0 if src_id == 1 else 1 \n",
        "                src_text = batches[src_id].text\n",
        "                progress_state = trainer.denoising_step(src_text, src_id)\n",
        "                logger.log(progress_state)\n",
        "                translation = trainer.generate_translation(src_text, src_id, trg_id)\n",
        "                progress_state = trainer.backtranslation_step(src_text, translation, src_id, trg_id)\n",
        "                logger.log(progress_state)\n",
        "\n",
        "            if trainer.n_total_iter % params.save_every_ith_iter == 0:\n",
        "                trainer.save_model(params.dump_dir, 'checkpoint-{}-{}'.format(trainer.epoch, trainer.n_total_iter))\n",
        "            \n",
        "            trainer.n_total_iter += 1\n",
        "\n",
        "        trainer.epoch += 1\n",
        "        # TODO add bleu scoring \n",
        "        # And saving best model\n",
        "        # Add loading model if it exists\n",
        "\n",
        "    log_file.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VW-iwnyek4Jq",
        "colab_type": "code",
        "outputId": "97635917-91c2-48f8-f0e8-b47b128af30c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        }
      },
      "source": [
        "class Parameters:\n",
        "  def __init__(self):\n",
        "    # Embeddings\n",
        "    self.embs_file = \"%s.vec\" % os.environ[\"BPE_EMBEDDINGS\"]\n",
        "    self.embs_dir = os.environ[\"EMBEDDINGS_DIR\"]\n",
        "    \n",
        "    # Dataset\n",
        "    self.l1_data_path = os.environ[\"L1_DATA_PREPARED\"]\n",
        "    self.l2_data_path = os.environ[\"L2_DATA_PREPARED\"]\n",
        "\n",
        "    self.l1_data_path = os.environ[\"L1_DATA_PREPARED\"] + '.cut'\n",
        "    self.l2_data_path = os.environ[\"L2_DATA_PREPARED\"] + '.cut'\n",
        "\n",
        "    # self.l1_vocab = \n",
        "    # self.l2_vocab = \n",
        "    \n",
        "    # Training\n",
        "    self.sequence_length = 256\n",
        "    self.batch_size = 64\n",
        "    self.lr = 0.25\n",
        "    self.clip = 1.0\n",
        "\n",
        "    self.n_epoch = 40\n",
        "    self.save_epoch = 5\n",
        "\n",
        "    self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    self.log_file = os.path.join(os.environ['MODELS'], 'log.txt')\n",
        "    self.dump_dir = os.environ['MODELS']\n",
        "\n",
        "    self.save_every_ith_iter = 1000\n",
        "\n",
        "\n",
        "main(Parameters())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-16f8017da483>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-16f8017da483>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# Embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membs_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"%s.vec\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"BPE_EMBEDDINGS\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membs_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"EMBEDDINGS_DIR\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSSU50AQKOSw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}