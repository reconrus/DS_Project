{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DS_Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4-final"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "CVGcr9jSPaVM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['PROJECT_PATH'] = os.path.abspath(os.curdir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zQtPdn2nPaVU"
      },
      "source": [
        "**Mount Google Drive** \n",
        "Tokenization will not work due to Perl script usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "R-wWLCVDPaVW"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "os.environ['PROJECT_PATH']='/content/ydrive/My Drive/Study/DS_Project'\n",
        "drive.mount('/content/ydrive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "dgoNDMrfPaVa"
      },
      "outputs": [],
      "source": [
        "os.environ['TOOLS']= os.path.join(os.environ['PROJECT_PATH'], 'tools')\n",
        "os.environ['RESOURCES']= os.path.join(os.environ['PROJECT_PATH'], 'resources')\n",
        "os.environ['DATA']= os.path.join(os.environ['RESOURCES'], 'data')\n",
        "os.environ['MODELS']= os.path.join(os.environ['PROJECT_PATH'], 'models')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RuxEswhD2CvL"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "S1IEhR76PaVg"
      },
      "source": [
        "### Download data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Bashkir language"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "oAFnrBfLPaVi"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "BASHKIR=\"$DATA/bashkir\"\n",
        "git clone https://github.com/nevmenandr/bashkir-corpus \"$BASHKIR-corpus\"\n",
        "mkdir \"$BASHKIR\" & mkdir \"$BASHKIR/raw\"\n",
        "find \"$BASHKIR-corpus\" -name \"*.txt\" -print0 | xargs -0 -I file cat file > \"$BASHKIR/ba\"\n",
        "rm -rf -d  \"$BASHKIR-corpus\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ['L1']='ba'\n",
        "os.environ['L2']='ru'\n",
        "os.environ['L1_DATA']=\"ba\"  \n",
        "os.environ['L2_DATA']=\"news.2017.et.shuffled.deduped\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AW5yovpH3MAn"
      },
      "source": [
        "### Text cleaning and tokenization\n",
        "**DOES NOT WORK IN COLAB DUE TO PERL SCRIPTS USAGE** \\\\\n",
        "[Source](https://github.com/facebookresearch/XLM/blob/master/tools/tokenize.sh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "MD6fHH8IPaVp"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "MOSES=\"$TOOLS/mosesdecoder\"\n",
        "git clone https://github.com/moses-smt/mosesdecoder \"$MOSES\"\n",
        "\n",
        "REPLACE_UNICODE_PUNCT=\"$MOSES/scripts/tokenizer/replace-unicode-punctuation.perl\"\n",
        "NORM_PUNC=\"$MOSES/scripts/tokenizer/normalize-punctuation.perl\"\n",
        "REM_NON_PRINT_CHAR=\"$MOSES/scripts/tokenizer/remove-non-printing-char.perl\"\n",
        "TOKENIZER=\"$MOSES/scripts/tokenizer/tokenizer.perl\"\n",
        "set -e\n",
        "function clean () { \\\n",
        "  cat - | \"$REPLACE_UNICODE_PUNCT\" | \"$NORM_PUNC\" -l $1 | \\\n",
        "          \"$REM_NON_PRINT_CHAR\" | \"$TOKENIZER\" \\\n",
        "          -no-escape -threads $(grep -c ^processor /proc/cpuinfo) -l $1;}\n",
        "\n",
        "cat \"$DATA/$L1_DATA\" | clean \"$L1\" > \"$DATA/${L1_DATA}.cleaned\"\n",
        "cat \"$DATA/$L2_DATA\" | clean \"$L2\" > \"$DATA/${L2_DATA}.cleaned\"\n",
        "\n",
        "# TODO change to https://github.com/alvations/sacremoses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V8tAgafPPaVy"
      },
      "source": [
        "### BPE codes generating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "x9mLvL5OPaV0"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "FASTBPE=\"$TOOLS/fastBPE\"\n",
        "FAST=\"$FASTBPE/fast\"\n",
        "git clone https://github.com/glample/fastBPE \"$FASTBPE\"\n",
        "\n",
        "g++ -std=c++11 -pthread -O3 \"$FASTBPE/main.cc\" -IfastBPE -o \"$FAST\"\n",
        "\"$FAST\" learnbpe 40000 \"$DATA/${L1_DATA}.cleaned\" \"$DATA/${L2_DATA}.cleaned\" > \"$DATA/BPE_codes\"\n",
        "\"$FAST\" applybpe \"$DATA/${L1_DATA}.40000\" \"$DATA/${L1_DATA}.cleaned\" \"$DATA/BPE_codes\"\n",
        "\"$FAST\" applybpe \"$DATA/${L2_DATA}.40000\" \"$DATA/${L2_DATA}.cleaned\" \"$DATA/BPE_codes\"\n",
        "\n",
        "### Vocabulary will be calculated after dataset loading\n",
        "# \"$FAST\" getvocab \"$DATA/${L1_DATA}.40000\" > \"$DATA/vocab.${L1_DATA}.40000\" \n",
        "# \"$FAST\" getvocab \"$DATA/${L2_DATA}.40000\" > \"$DATA/vocab.${L2_DATA}.40000\" \n",
        "\n",
        "# TODO Add splitting data on train, valid and test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cKR2NM2JOxSe"
      },
      "source": [
        "### N-gram Translation Table Inferring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "cWs7DxH_PIPm"
      },
      "outputs": [],
      "source": [
        "![ -f $RESOURCES/cc.ba.300.vec ] || wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ba.300.vec.gz -P  \"$RESOURCES\"\n",
        "![ -f $RESOURCES/cc.ru.300.vec ] || wget -nc https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ru.300.vec.gz -P \"$RESOURCES\"\n",
        "!gzip -d \"$RESOURCES/cc.ba.300.vec.gz\"\n",
        "!gzip -d \"$RESOURCES/cc.ru.300.vec.gz\"\n",
        "!git clone https://github.com/artetxem/vecmap.git \"$TOOLS\"\n",
        "!python3 \"$TOOLS/map_embeddings.py\" --unsupervised \"$RESOURCES/cc.ba.300.vec\" \"$RESOURCES/cc.ru.300.vec\" \"$RESOURCES/ba_mapped.vec\" \"$RESOURCES/ru_mapped.vec\" \n",
        "\n",
        "# TODO add the table inferring from above cross-lingual embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WGIdhpFGvvil"
      },
      "source": [
        "## Model implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ykOAbsio60I6"
      },
      "source": [
        "### Tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "lwfdjGo6Wvw-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "#src https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "owkE0F9fFSBW"
      },
      "source": [
        "### BaseModel\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_WY8LLAnjI_b"
      },
      "source": [
        "Temporarily hyperparameters are hardcoded for better readability \n",
        "\n",
        "\n",
        "Sources:\n",
        "1. https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
        "2. https://github.com/facebookresearch/XLM/blob/master/src/model/transformer.py\n",
        "3. https://discuss.pytorch.org/t/memory-mask-in-nn-transformer/55230/5\n",
        "4. https://github.com/tkmaroon/pytorch-xlm/blob/master/models/transformer.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "z523fnIeFRhd"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "from torch.nn import TransformerDecoder, TransformerDecoderLayer, \\\n",
        "                     TransformerEncoder, TransformerEncoderLayer\n",
        "\n",
        "\n",
        "class BaseModel(nn.Module):\n",
        "    def __init__(self, field, d_model=1024, nlayers=6, nheads=8, dropout=0.1):\n",
        "        super(BaseModel, self).__init__()\n",
        "        \n",
        "        # [4]\n",
        "        self.voc_size = len(field.vocab.itos) \n",
        "        self.pad_idx = field.vocab.stoi['<pad>']\n",
        "        self.bos_idx = field.vocab.stoi['<bos>']\n",
        "        self.eos_idx = field.vocab.stoi['<eos>']\n",
        "        self.sep_idx = field.vocab.stoi['<sep>']\n",
        "        self.mask_idx = field.vocab.stoi['<mask>']\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.dropout = dropout\n",
        "        self.embedding = nn.Embedding(self.voc_size, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model)\n",
        "        encoder_layers = TransformerEncoderLayer(d_model, nheads, dim_feedforward=4*d_model, activation='gelu')\n",
        "        self.encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        nn.init.normal_(self.embedding.weight, mean=0, std=self.d_model ** -0.5) #[2] L46\n",
        "\n",
        "    def forward(self, src):\n",
        "        pad_mask = torch.transpose(src.eq(self.pad_idx), 0, 1)\n",
        "        src_mask = self.get_mask(src, pad_mask)\n",
        "        src = self.embedding(src)\n",
        "        src = self.pos_encoder(src)\n",
        "        src = self.encoder(src)\n",
        "        src = F.dropout(src, self.dropout, training=self.training)\n",
        "\n",
        "        output = self.encoder(src, src_mask, pad_mask)\n",
        "        return output\n",
        "\n",
        "\n",
        "    def get_mask(self, inputs, pad_mask): #[2]\n",
        "        slen, bs = inputs.size()\n",
        "        lengths = slen-torch.sum(pad_mask, 0)\n",
        "\n",
        "        alen = torch.arange(slen, dtype=torch.long, device=lengths.device)\n",
        "\n",
        "        return alen < lengths[:, None]\n",
        "\n",
        "    # [4]\n",
        "    def mlm_loss(self, inputs, criterion, sampling_rate=0.15, masked_rate=0.8, replaced_rate=0.1, unchanged_rate=0.1):\n",
        "        slen, bs = inputs.size()\n",
        "\n",
        "        sampler = self.sampling(inputs, sampling_rate)\n",
        "        rnd = torch.rand((slen, bs), device=inputs.device)\n",
        "        mask = (masked_rate >= rnd) & sampler\n",
        "\n",
        "        # replace mask tokens\n",
        "        inputs = torch.where(\n",
        "            (masked_rate >= rnd) & mask,\n",
        "            torch.ones_like(inputs) * self.mask_idx, \n",
        "            inputs,\n",
        "        )\n",
        "\n",
        "        # replace random tokens\n",
        "        th = masked_rate + replaced_rate\n",
        "        inputs = torch.where(\n",
        "            (th >= rnd) & (rnd > masked_rate) & sampler, \n",
        "            torch.randint_like(inputs, self.mask_idx+1, self.voc_size),\n",
        "            inputs\n",
        "        )\n",
        "\n",
        "        outs = self.forward(inputs).view(slen*bs, -1)\n",
        "        loss = criterion(outs, inputs.view(-1))\n",
        "        return loss\n",
        "\n",
        "    # [4]\n",
        "    def sampling(self, inputs, sampling_rate):\n",
        "        slen, bs = inputs.size()\n",
        "        rnd = -torch.rand((slen, bs))\n",
        "        mask = rnd.ge(-sampling_rate)\n",
        "        mask[inputs <= self.mask_idx] = 0 # special tokens are not sampled\n",
        "        return mask.to(inputs.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kSSoHD9rNX_5"
      },
      "source": [
        "### Pretraining"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "TSpdyLWxNbXW"
      },
      "outputs": [],
      "source": [
        "!pip install torchtext==0.5.0\n",
        "import math\n",
        "from collections import OrderedDict\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext import data, datasets\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, model, criterion, optimizer, clip, n_iter=0):\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.clip = clip\n",
        "        self.n_updates = n_iter\n",
        "\n",
        "    def get_lr(self):\n",
        "        return self.optimizer.param_groups[0]['lr']\n",
        "\n",
        "    def step(self, inputs):\n",
        "        self.optimizer.zero_grad()\n",
        "        loss = self.model.mlm_loss(inputs, self.criterion)\n",
        "\n",
        "        if self.model.training:\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(self.model.parameters(), self.clip)\n",
        "            self.optimizer.step()\n",
        "            self.n_updates += 1\n",
        "        return loss\n",
        "    \n",
        "\n",
        "def pretrain(data_path, device, min_vocab_freq=1, batch_size=64, sequence_length=256, lr=0.25, clip=1.0, n_epoch=40, save_epoch=2):\n",
        "\n",
        "    TEXT = data.Field(\n",
        "        init_token='<bos>', \n",
        "        eos_token='<eos>',\n",
        "    )\n",
        "\n",
        "    train_dataset = datasets.LanguageModelingDataset(data_path, TEXT)\n",
        "\n",
        "    vocab = TEXT.build_vocab(\n",
        "        train_dataset, \n",
        "        min_freq=min_vocab_freq, \n",
        "        specials=['<sep>', '<mask>']\n",
        "    )\n",
        "    train_iter = data.BPTTIterator(\n",
        "        train_dataset, \n",
        "        batch_size=batch_size, \n",
        "        bptt_len=256,\n",
        "        train=True, \n",
        "        repeat=False, \n",
        "        shuffle=True,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    if not os.path.exists(os.environ['MODELS']):\n",
        "        os.mkdir(os.environ['MODELS'])\n",
        "    \n",
        "    model = BaseModel(TEXT).to(device)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=TEXT.vocab.stoi['<pad>'])\n",
        "    optimizer = Adam(model.parameters(), lr=lr)\n",
        "    best_loss = math.inf\n",
        "    epoch=0\n",
        "    trainer = Trainer(model, criterion, optimizer, clip)\n",
        "\n",
        "    while epoch <= n_epoch:\n",
        "        # training\n",
        "        with tqdm(train_iter, dynamic_ncols=True) as pbar:\n",
        "            train_loss = 0.0\n",
        "            model.train()\n",
        "            for samples in pbar:\n",
        "                srcs = samples.text.to(device)\n",
        "                loss = trainer.step(srcs)\n",
        "                train_loss += loss.item()\n",
        "\n",
        "                # setting of progressbar\n",
        "                pbar.set_description(f'epoch {str(epoch).zfill(3)}')\n",
        "                progress_state = OrderedDict(\n",
        "                    loss=loss.item(),\n",
        "                    bsz=srcs.size(1),\n",
        "                    lr=trainer.get_lr(), \n",
        "                    clip=clip, \n",
        "                    num_updates=trainer.n_updates)\n",
        "                pbar.set_postfix(progress_state)\n",
        "        train_loss /= len(train_iter)\n",
        "\n",
        "        print(f'| epoch {str(epoch).zfill(3)} | train ', end='') \n",
        "        print(f'| loss {train_loss:.{4}} ', end='')\n",
        "        print(f'| lr {trainer.get_lr():.1e} ', end='')\n",
        "        print(f'| clip {clip} ', end='')\n",
        "        print(f'| num_updates {trainer.n_updates} |')\n",
        "        \n",
        "        # saving model\n",
        "        save_vars = {\n",
        "            'epoch': epoch,\n",
        "            'iteration': trainer.n_updates,\n",
        "            'best_loss': train_loss if train_loss < best_loss else best_loss,\n",
        "            'weights': model.state_dict()\n",
        "        }\n",
        "\n",
        "        if train_loss < best_loss:\n",
        "            best_loss = train_loss\n",
        "            filename = os.path.join(os.environ['MODELS'], 'checkpoint_best.pt') \n",
        "            torch.save(save_vars, filename)\n",
        "        if epoch % save_epoch == 0:\n",
        "            filename = os.path.join(os.environ['MODELS'], f'checkpoint_{epoch}.pt') \n",
        "            torch.save(save_vars, filename)\n",
        "        filename = os.path.join(os.environ['MODELS'], 'checkpoint_last.pt') \n",
        "        torch.save(save_vars, filename)\n",
        "\n",
        "        epoch += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "yeiC8PqvPaWL"
      },
      "outputs": [],
      "source": [
        "# l1_data_path = os.path.join(os.environ['DATA'], \"%s.cleaned\" % os.environ['L1_DATA'])\n",
        "# !head -64 \"${DATA}/${L1_DATA}.cleaned\" > \"${DATA}/${L1_DATA}.cut\"\n",
        "\n",
        "l1_data_path = os.path.join(os.environ['DATA'], \"%s.cut\" % os.environ['L1_DATA'])\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "pretrain(l1_data_path, device, n_epoch=1)"
      ]
    }
  ]
}