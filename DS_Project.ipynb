{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PROJECT_PATH'] = os.path.abspath(os.curdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mount Google Drive** \n",
    "Tokenization will not work due to Perl script usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# os.environ['PROJECT_PATH']='/content/ydrive/My Drive/Study/DS_Project'\n",
    "# drive.mount('/content/ydrive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TOOLS']='%s/tools' % os.environ['PROJECT_PATH']\n",
    "os.environ['RESOURCES']='%s/resources' % os.environ['PROJECT_PATH']\n",
    "os.environ['DATA']='%s/data' % os.environ['RESOURCES']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RuxEswhD2CvL"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "--2020-03-14 18:47:07--  http://data.statmt.org/wmt18/translation-task/news.2017.cs.shuffled.deduped.gz\nResolving data.statmt.org (data.statmt.org)...129.215.197.184\nConnecting to data.statmt.org (data.statmt.org)|129.215.197.184|:80...connected.\nHTTP request sent, awaiting response...200 OK\nLength: 337728864 (322M) [application/x-gzip]\nSaving to: ‘/home/hawk/Programming/Study/DS_Project/DS_Project/resources/data/news.2017.cs.shuffled.deduped.gz’\n\n                      5%[>                   ]  16,42M  1,87MB/s    eta 2m 48s^C\n--2020-03-14 18:47:17--  http://data.statmt.org/wmt18/translation-task/news.2017.en.shuffled.deduped.gz\nResolving data.statmt.org (data.statmt.org)... 129.215.197.184\nConnecting to data.statmt.org (data.statmt.org)|129.215.197.184|:80... connected.\nHTTP request sent, awaiting response...200 OK\nLength: 1387271195 (1,3G) [application/x-gzip]\nSaving to: ‘/home/hawk/Programming/Study/DS_Project/DS_Project/resources/data/news.2017.en.shuffled.deduped.gz’\n\n              news.   9%[>                   ] 126,88M  1,73MB/s    eta 12m 58s"
    }
   ],
   "source": [
    " ![ -f \"$RESOURCES/data/news.2017.cs.shuffled\" ] || wget http://data.statmt.org/wmt18/translation-task/news.2017.cs.shuffled.deduped.gz -P \"$RESOURCES/data\"\n",
    "![ -f \"$RESOURCES/data/news.2017.en.shuffled\" ] || wget http://data.statmt.org/wmt18/translation-task/news.2017.en.shuffled.deduped.gz -P \"$RESOURCES/data\"\n",
    "!gzip -d \"$RESOURCES/data/news.2017.cs.shuffled.deduped.gz\"\n",
    "!gzip -d \"$RESOURCES/data/news.2017.en.shuffled.deduped.gz\"\n",
    "os.environ['L1']='cs'\n",
    "os.environ['L2']='en'\n",
    "os.environ['L1_DATA']=\"news.2017.cs.shuffled\"  \n",
    "os.environ['L2_DATA']=\"news.2017.en.shuffled\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AW5yovpH3MAn"
   },
   "source": [
    "### Text cleaning and tokenization\n",
    "[Source](https://github.com/facebookresearch/XLM/blob/master/tools/tokenize.sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "fatal: destination path '/home/hawk/Programming/Study/DS_Project/DS_Project/tools/mosesdecoder' already exists and is not an empty directory.\nTokenizer Version 1.1\nLanguage: en\nNumber of threads: 8\n"
    }
   ],
   "source": [
    "%%bash\n",
    "MOSES=\"$TOOLS/mosesdecoder\"\n",
    "git clone https://github.com/moses-smt/mosesdecoder \"$MOSES\"\n",
    "\n",
    "REPLACE_UNICODE_PUNCT=\"$MOSES/scripts/tokenizer/replace-unicode-punctuation.perl\"\n",
    "NORM_PUNC=\"$MOSES/scripts/tokenizer/normalize-punctuation.perl\"\n",
    "REM_NON_PRINT_CHAR=\"$MOSES/scripts/tokenizer/remove-non-printing-char.perl\"\n",
    "TOKENIZER=\"$MOSES/scripts/tokenizer/tokenizer.perl\"\n",
    "set -e\n",
    "function clean () { \\\n",
    "  cat - | \"$REPLACE_UNICODE_PUNCT\" | \"$NORM_PUNC\" -l $1 | \\\n",
    "          \"$REM_NON_PRINT_CHAR\" | \"$TOKENIZER\" \\\n",
    "          -no-escape -threads $(grep -c ^processor /proc/cpuinfo) -l $1;}\n",
    "\n",
    "# \"$DATA/$L1_DATA\" | clean \"$L1\" > \"$DATA/${L1_DATA}.cleaned\"\n",
    "# \"$DATA/$L2_DATA\" | clean \"$L2\" > \"$DATA/${L2_DATA}.cleaned\"\n",
    "\n",
    "# Usage example\n",
    "head \"$DATA/news.2017.et.shuffled\" | clean en > \"$DATA/news.2017.et.shuffled.cleaned\" &"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPE codes generating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "UsageError: Cell magic `%%shell` not found.\n"
    }
   ],
   "source": [
    "%%bash\n",
    "FASTBPE=\"$TOOLS/fastBPE\"\n",
    "FAST=\"$FASTBPE/fast\"\n",
    "git clone https://github.com/glample/fastBPE \"$FASTBPE\"\n",
    "\n",
    "g++ -std=c++11 -pthread -O3 \"$FASTBPE/main.cc\" -IfastBPE -o \"$FAST\"\n",
    "\"$FAST\" learnbpe 40000 \"$DATA/${L1_DATA}.cleaned\" \"$DATA/${L2_DATA}.cleaned\" > \"$DATA/BPE_codes\"\n",
    "\"$FAST\" applybpe \"$DATA/${L1_DATA}.40000\" \"$DATA/${L1_DATA}.cleaned\" \"$DATA/BPE_codes\"\n",
    "\"$FAST\" applybpe \"$DATA/${L2_DATA}.40000\" \"$DATA/${L2_DATA}.cleaned\" \"$DATA/BPE_codes\"\n",
    "\n",
    "\"$FAST\" getvocab \"$DATA/${L1_DATA}.40000\" > \"$DATA/vocab.${L1_DATA}.40000\" \n",
    "\"$FAST\" getvocab \"$DATA/${L2_DATA}.40000\" > \"$DATA/vocab.${L2_DATA}.40000\" \n",
    "\n",
    "# Add splitting data on train, valid and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cKR2NM2JOxSe"
   },
   "source": [
    "### N-gram Translation Table Inferring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "colab_type": "code",
    "id": "cWs7DxH_PIPm",
    "outputId": "616c03a7-8361-4089-8d94-ee620b99ff1d"
   },
   "outputs": [],
   "source": [
    "![ -f $RESOURCES/cc.cs.300.vec ] || wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.cs.300.vec.gz -P  \"$RESOURCES\"\n",
    "![ -f $RESOURCES/cc.en.300.vec ] || wget -nc https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz -P \"$RESOURCES\"\n",
    "!gzip -d \"$RESOURCES/cc.cs.300.vec.gz\"\n",
    "!gzip -d \"$RESOURCES/cc.en.300.vec.gz\"\n",
    "!git clone https://github.com/artetxem/vecmap.git \"$TOOLS\"\n",
    "!python3 \"$TOOLS/map_embeddings.py\" --unsupervised \"$RESOURCES/cc.cs.300.vec\" \"$RESOURCES/cc.en.300.vec\" \"$RESOURCES/cs_mapped.vec\" \"$RESOURCES/en_mapped.vec\" \n",
    "\n",
    "# TODO add the table inferring from above cross-lingual embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WGIdhpFGvvil"
   },
   "source": [
    "## Model implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ykOAbsio60I6"
   },
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lwfdjGo6Wvw-"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "#src https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "owkE0F9fFSBW"
   },
   "source": [
    "### BaseModel\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_WY8LLAnjI_b"
   },
   "source": [
    "Temporarily hyperparameters are hardcoded for better readability \n",
    "\n",
    "\n",
    "Sources:\n",
    "1. https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "2. https://github.com/facebookresearch/XLM/blob/master/src/model/transformer.py\n",
    "3. https://discuss.pytorch.org/t/memory-mask-in-nn-transformer/55230/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z523fnIeFRhd"
   },
   "outputs": [],
   "source": [
    "from torch.nn import TransformerDecoder, TransformerDecoderLayer,\\\\\n",
    "                     TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "def get_masks(slen, lengths, causal): #[2]\n",
    "    \"\"\"\n",
    "    Generate hidden states mask, and optionally an attention mask.\n",
    "    \"\"\"\n",
    "    bs = lengths.size(0)\n",
    "    alen = torch.arange(slen, dtype=torch.long, device=lengths.device)\n",
    "    mask = alen < lengths[:, None]\n",
    "\n",
    "    # attention mask is the same as mask, or triangular inferior attention (causal)\n",
    "    if causal:\n",
    "        attn_mask = alen[None, None, :].repeat(bs, slen, 1) <= alen[None, :, None]\n",
    "    else:\n",
    "        attn_mask = mask\n",
    "\n",
    "    return mask, attn_mask\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, voc_size, d_model, nlayers=6, nheads=8,):\n",
    "        super(BaseModel, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(voc_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nheads, dim_feedforward=1024, activation='gelu')\n",
    "        self.encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.embedding.init.normal_(self.embedding.weight, mean=0, std=self.d_model ** -0.5) #[2] L46\n",
    "\n",
    "    def forward(self, src, src_lengths, trg):\n",
    "        src_len, bs = src.size()\n",
    "\n",
    "        src_mask, _ = get_masks(src, src_lengths, False) # TODO check causality parameter\n",
    "\n",
    "        src = self.encoder(src)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.encoder(src, src_mask)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, voc_size, d_model, nlayers=6, nheads=8,):\n",
    "        super(BaseModel, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(voc_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nheads, dim_feedforward=1024, activation='gelu')\n",
    "        self.encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.embedding.init.normal_(self.embedding.weight, mean=0, std=self.d_model ** -0.5) #[2] L46\n",
    "\n",
    "    def forward(self, src, src_lengths, trg):\n",
    "        src_len, bs = src.size()\n",
    "\n",
    "        src_mask, _ = get_masks(src, src_lengths, False) # TODO check causality parameter\n",
    "\n",
    "        src = self.encoder(src)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.encoder(src, src_mask)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kSSoHD9rNX_5"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TSpdyLWxNbXW"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def training(l1_training, l2_training, lambda):\n",
    "    \n",
    "    x, lengths, positions, langs = generate_batch(lang1, lang2, 'pred')\n",
    "    \n",
    "\n",
    "        params = self.params\n",
    "        name = 'model' if params.encoder_only else 'encoder'\n",
    "        model = getattr(self, name)\n",
    "        model.train()\n",
    "\n",
    "        # generate batch / select words to predict\n",
    "        x, lengths, positions, langs, _ = self.generate_batch(lang1, lang2, 'pred')\n",
    "        x, lengths, positions, langs, _ = self.round_batch(x, lengths, positions, langs)\n",
    "        x, y, pred_mask = self.mask_out(x, lengths)\n",
    "\n",
    "        # cuda\n",
    "        x, y, pred_mask, lengths, positions, langs = to_cuda(x, y, pred_mask, lengths, positions, langs)\n",
    "\n",
    "        # forward / loss\n",
    "        tensor = model('fwd', x=x, lengths=lengths, positions=positions, langs=langs, causal=False)\n",
    "        _, loss = model('predict', tensor=tensor, pred_mask=pred_mask, y=y, get_scores=False)\n",
    "        self.stats[('MLM-%s' % lang1) if lang2 is None else ('MLM-%s-%s' % (lang1, lang2))].append(loss.item())\n",
    "        loss = lambda_coeff * loss\n",
    "\n",
    "        # optimize\n",
    "        self.optimize(loss)\n",
    "\n",
    "        # number of processed sentences / words\n",
    "        self.n_sentences += params.batch_size\n",
    "        self.stats['processed_s'] += lengths.size(0)\n",
    "        self.stats['processed_w'] += pred_mask.sum().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s2FARz8r67-A"
   },
   "source": [
    "# Archive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-aG7NhuRQxXD"
   },
   "source": [
    "### Encoder/Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xH5J48Sw7cAn"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    self.src_word_emds = nn.Embedding(n_src_vocab, d_word_vec, padding_idx=pad_idx)\n",
    "    self.position_enc = PositionalEncoding(d_word_vec, n_position=n_position)\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.trg_word_emds = create_emb_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_gb6Yu3hQqVw"
   },
   "source": [
    "### BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-LW_PS79669V"
   },
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "\n",
    "    def __init__(self, voc_size, d_model, nlayers=6, nheads=8,):\n",
    "        super(BaseModel, self).__init__()\n",
    "        # TODELETE\n",
    "        # self.src_mask = None\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(voc_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nheads, dim_feedforward=1024, activation='gelu')\n",
    "        self.encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "\n",
    "\n",
    "        # decoder_layers = TransformerDecoderLayer(d_model, nheads, dim_feedforward=1024, activation='gelu')\n",
    "        # self.decoder = nn.TransformerDecoder(d_model, nlayers)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz): #[1]\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def _generate_memory_mask(trg_sz, src_sz): #[3]\n",
    "        mask = (torch.triu(torch.ones(src_sz, trg_sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.embedding.init.normal_(self.embedding.weight, mean=0, std=self.d_model ** -0.5) #[2] L46\n",
    "\n",
    "    def forward(self, src, src_lengths, trg):\n",
    "        # TODELETE\n",
    "        # if self.src_mask is None or self.src_mask.size(0) != len(src): #[1]\n",
    "        #     device = src.device\n",
    "        #     self.src_mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "\n",
    "        src_len, bs = src.size()\n",
    "\n",
    "        src_mask, attn_mask = get_masks(src, src_lengths, False) # TODO check causality parameter\n",
    "\n",
    "        # TODELETE\n",
    "        src = self.encoder(src) #* math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.encoder(src, src_mask)\n",
    "\n",
    "        return output\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "s2FARz8r67-A"
   ],
   "name": "Untitled",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}